#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Dialog Analyzer v5.3 ROBUST PARSING + COMPREHENSIVE TRANSITIONS
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ —Å –Ω–∞–¥—ë–∂–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º –Ω–µ–≤–∞–ª–∏–¥–Ω–æ–≥–æ JSONL
"""

import os
import sys
import json

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ utils
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from utils.config import *
from utils.loaders import load_intents
from utils.validators import run_all_validations, save_validation_report
from utils.analyzers import first_pass, second_pass, third_pass, fourth_pass

# Import graph analyzer
try:
    from utils.graph_analyzer import analyze_graph_structure
    GRAPH_ANALYSIS_AVAILABLE = True
except ImportError:
    GRAPH_ANALYSIS_AVAILABLE = False
    print("‚ö†Ô∏è  Graph analysis module not available")

# Import risk analyzer
try:
    from utils.risk_analyzer import (
        analyze_intent_risks, generate_risk_summary,
        generate_risk_legend, export_risk_report
    )
    RISK_ANALYSIS_AVAILABLE = True
except ImportError:
    RISK_ANALYSIS_AVAILABLE = False
    print("‚ö†Ô∏è  Risk analysis module not available")

# Import quality analyzers
try:
    from utils.regex_analyzer import analyze_intent_regex_patterns
    REGEX_ANALYSIS_AVAILABLE = True
except ImportError:
    REGEX_ANALYSIS_AVAILABLE = False
    print("‚ö†Ô∏è  Regex analysis module not available")

try:
    from utils.entry_point_analyzer import analyze_entry_points
    ENTRY_POINT_ANALYSIS_AVAILABLE = True
except ImportError:
    ENTRY_POINT_ANALYSIS_AVAILABLE = False
    print("‚ö†Ô∏è  Entry point analysis module not available")

try:
    from utils.freshness_analyzer import analyze_data_freshness, get_update_distribution
    FRESHNESS_ANALYSIS_AVAILABLE = True
except ImportError:
    FRESHNESS_ANALYSIS_AVAILABLE = False
    print("‚ö†Ô∏è  Freshness analysis module not available")

try:
    from utils.diagram_exporter import export_mermaid_graph, export_detailed_flow_diagram
    DIAGRAM_EXPORT_AVAILABLE = True
except ImportError:
    DIAGRAM_EXPORT_AVAILABLE = False
    print("‚ö†Ô∏è  Diagram export module not available")

try:
    from utils.multi_format_exporter import export_all_formats
    MULTI_FORMAT_EXPORT_AVAILABLE = True
except ImportError:
    MULTI_FORMAT_EXPORT_AVAILABLE = False
    print("‚ö†Ô∏è  Multi-format export module not available")

def print_header():
    """–ü–µ—á–∞—Ç—å –∫—Ä–∞—Å–∏–≤–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
    print()
    print("=" * 80)
    print("üöÄ DIALOG ANALYZER v5.3 - COMPREHENSIVE TRANSITIONS")
    print("=" * 80)
    print("üìú –†–µ–∂–∏–º: Read-Only Analysis with Robust JSONL Parsing")
    print("üõ°Ô∏è  –î–∞–Ω–Ω—ã–µ –Ω–µ –∏–∑–º–µ–Ω—è—é—Ç—Å—è - —Ç–æ–ª—å–∫–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏")
    print("üîß –ù–û–í–û–ï: –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω–æ–≥–æ JSONL (Extra data, multiple objects)")
    print("üìä –í–ö–õ–Æ–ß–ï–ù–û: –†–∏—Å–∫–∏, –≥—Ä–∞—Ñ, –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞, –≤—Å–µ —Ç–∏–ø—ã –ø–µ—Ä–µ—Ö–æ–¥–æ–≤")
    print()

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    print_header()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    print("=" * 80)
    print("üìÖ –≠–¢–ê–ü 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    print("=" * 80)
    
    if not os.path.exists(INPUT_FILE):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {INPUT_FILE}")
        print(f"üí° –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª {INPUT_FILE} –∏–ª–∏ —É–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –≤ utils/config.py")
        print()
        print("üìù –û–∂–∏–¥–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: JSONL (–ø–æ –æ–¥–Ω–æ–º—É JSON –Ω–∞ —Å—Ç—Ä–æ–∫—É)")
        print("   –ü—Ä–∏–º–µ—Ä:")
        print('   {"intent_id": "1", "title": "Test", ...}')
        print('   {"intent_id": "2", "title": "Test 2", ...}')
        return 1
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    intents, metadata = load_intents(INPUT_FILE, MAX_LINES)
    
    if not intents:
        print()
        print("=" * 80)
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        print("=" * 80)
        print()
        print("üìù –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("   1. –§–∞–π–ª –ø—É—Å—Ç–æ–π")
        print("   2. –í—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–º–µ—é—Ç –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON")
        print("   3. –§–∞–π–ª –Ω–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON/JSONL")
        print()
        print("üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ —Ñ–∞–π–ª–∞:")
        try:
            with open(INPUT_FILE, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if i >= 3:
                        break
                    print(f"   Line {i+1}: {line[:80]}...")
        except Exception as e:
            print(f"   –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {e}")
        print()
        return 1
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏
    print()
    print("=" * 80)
    print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏")
    print("=" * 80)
    print(f"‚úÖ –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {metadata.get('total_loaded', 0)} –∏–Ω—Ç–µ–Ω—Ç–æ–≤")
    print(f"üìù –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: {metadata.get('total_lines_processed', 0)}")
    
    parsing_stats = metadata.get('parsing_stats', {})
    if parsing_stats:
        print()
        print("üîç –î–µ—Ç–∞–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞:")
        print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ: {parsing_stats.get('success', 0)}")
        if parsing_stats.get('fixed_extra_data', 0) > 0:
            print(f"   üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ (Extra data): {parsing_stats['fixed_extra_data']}")
        if parsing_stats.get('skipped_empty', 0) > 0:
            print(f"   ‚ö™ –ü—Ä–æ–ø—É—â–µ–Ω–æ (–ø—É—Å—Ç—ã–µ): {parsing_stats['skipped_empty']}")
        if parsing_stats.get('skipped_invalid', 0) > 0:
            print(f"   ‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON): {parsing_stats['skipped_invalid']}")
    
    if 'filtered_expired' in metadata:
        print(f"   üóëÔ∏è  –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –∏—Å—Ç—ë–∫—à–∏—Ö: {metadata['filtered_expired']}")
    
    print(f"   üì¶ –§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {metadata.get('final_count', 0)}")
    
    version_stats = metadata.get('version_statistics', {})
    if version_stats and any(version_stats.values()):
        print()
        print("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ—Ä—Å–∏–π:")
        print(f"   –° –≤–µ—Ä—Å–∏–µ–π: {version_stats.get('with_version', 0)}")
        print(f"   –° expire: {version_stats.get('with_expire', 0)}")
        print(f"   –ê–∫—Ç–∏–≤–Ω—ã—Ö: {version_stats.get('active', 0)}")
        print(f"   –ò—Å—Ç—ë–∫—à–∏—Ö: {version_stats.get('expired', 0)}")
    
    # 2. –í–∞–ª–∏–¥–∞—Ü–∏—è
    validation_results = {}
    if ENABLE_VALIDATION:
        print()
        print("=" * 80)
        print("üîç –≠–¢–ê–ü 2: –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")
        print("=" * 80)
        validation_results = run_all_validations(intents, {})
        
        if STOP_ON_VALIDATION_ERRORS and not validation_results['summary']['is_valid']:
            print()
            print("‚ùå –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏–∑-–∑–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
            print("üí° –û—Ç–∫–ª—é—á–∏—Ç–µ STOP_ON_VALIDATION_ERRORS –≤ config.py –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è")
            return 1
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        save_validation_report(validation_results, OUTPUT_DIR)
        print(f"üìÑ –û—Ç—á—ë—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {OUTPUT_DIR}/validation_report.json")
    
    # 3. –ê–Ω–∞–ª–∏–∑ (4 –ø—Ä–æ—Ö–æ–¥–∞)
    print()
    print("=" * 80)
    print("üî¨ –≠–¢–ê–ü 3: –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (4 –ø—Ä–æ—Ö–æ–¥–∞)")
    print("=" * 80)
    
    all_data = first_pass(intents)
    all_data = second_pass(intents, all_data)
    all_data = third_pass(intents, all_data)
    all_data = fourth_pass(intents, all_data)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã Transition
    transitions_full = all_data.get('transitions', [])
    # –î–ª—è graph_analyzer –Ω—É–∂–Ω—ã –∫–æ—Ä—Ç–µ–∂–∏
    transitions_tuples = [(t.source_id, t.target_id) for t in transitions_full]

    # 4. –ê–Ω–∞–ª–∏–∑ –≥—Ä–∞—Ñ–∞
    if GRAPH_ANALYSIS_AVAILABLE and ENABLE_VALIDATION:
        redirect_map = validation_results.get('redirects', {}).get('redirect_map', {})
        graph_analysis = analyze_graph_structure(intents, redirect_map, transitions_tuples)
        all_data['graph_analysis'] = graph_analysis
        validation_results['graph_analysis'] = graph_analysis

    # 5. –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    quality_metrics = {}
    print()
    print("=" * 80)
    print("üìä –≠–¢–ê–ü 4: –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö")
    print("=" * 80)

    if REGEX_ANALYSIS_AVAILABLE:
        regex_analysis = analyze_intent_regex_patterns(intents)
        quality_metrics['regex_complexity'] = regex_analysis

    if ENTRY_POINT_ANALYSIS_AVAILABLE:
        entry_point_analysis = analyze_entry_points(intents)
        quality_metrics['entry_points'] = entry_point_analysis

    if FRESHNESS_ANALYSIS_AVAILABLE:
        freshness_analysis = analyze_data_freshness(intents)
        if freshness_analysis.get('has_version_data'):
            update_dist = get_update_distribution(intents)
            freshness_analysis['update_distribution'] = update_dist
        quality_metrics['data_freshness'] = freshness_analysis

    # 6. –ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤
    if RISK_ANALYSIS_AVAILABLE and ENABLE_VALIDATION:
        print()
        print("=" * 80)
        print("üõ°Ô∏è  –≠–¢–ê–ü 5: –ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤")
        print("=" * 80)

        intent_risks = analyze_intent_risks(intents, validation_results)
        risk_summary = generate_risk_summary(intent_risks)

        risk_score = risk_summary['risk_score']
        if risk_score >= 80:
            score_icon = "‚úÖ"
        elif risk_score >= 60:
            score_icon = "üü°"
        elif risk_score >= 40:
            score_icon = "üü†"
        else:
            score_icon = "üî¥"

        print(f"\n{score_icon} –û–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥ —Ä–∏—Å–∫–æ–≤: {risk_score}/100")

        print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–≤–Ω—è–º —Ä–∏—Å–∫–∞:")
        severity_dist = risk_summary['severity_distribution']
        for severity in ['critical', 'high', 'medium', 'low', 'info']:
            count = severity_dist.get(severity, 0)
            if count > 0:
                pct = round(count / risk_summary['total_intents'] * 100, 1)
                print(f"   {severity.upper():10s}: {count:4d} ({pct}%)")

        critical_intents = risk_summary['critical_intents']
        if critical_intents:
            print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–ù–¢–ï–ù–¢–´ ({len(critical_intents)}):")
            for intent_id in critical_intents[:5]:
                risk_obj = intent_risks[intent_id]
                print(f"   - {intent_id}")
                for _, desc in risk_obj.risks[:2]:
                    print(f"      ‚Ä¢ {desc}")
            if len(critical_intents) > 5:
                print(f"   ... –∏ –µ—â—ë {len(critical_intents) - 5}")

        print(generate_risk_legend())

        risk_report_path = os.path.join(OUTPUT_DIR, 'risk_analysis.json')
        export_risk_report(intent_risks, risk_report_path)

        if quality_metrics:
            with open(risk_report_path, 'r', encoding='utf-8') as f:
                report = json.load(f)
            report['quality_metrics'] = quality_metrics
            with open(risk_report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ –æ—Ç—á—ë—Ç")

        all_data['intent_risks'] = intent_risks
        all_data['quality_metrics'] = quality_metrics

    # 6.1 Diagram export - –ü–µ—Ä–µ–¥–∞—ë–º –ø–æ–ª–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã Transition
    if EXPORT_DIAGRAMS and DIAGRAM_EXPORT_AVAILABLE:
        print()
        print("=" * 80)
        print("üñåÔ∏è  –≠–¢–ê–ü 6: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏–∞–≥—Ä–∞–º–º")
        print("=" * 80)
        
        # Mermaid –¥–∏–∞–≥—Ä–∞–º–º—ã (–¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –≥—Ä–∞—Ñ–æ–≤)
        if len(intents) <= 1000:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ –≥—Ä–∞—Ñ–∞
            diagram_path = os.path.join(OUTPUT_DIR, "graph.mmd")
            export_mermaid_graph(
                intents=intents,
                transitions=transitions_full,  # –ü–æ–ª–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã!
                intent_risks=all_data.get('intent_risks'),
                output_path=diagram_path,
                include_legend=INCLUDE_LEGEND,
            )
            print(f"\nüñåÔ∏è  Mermaid –¥–∏–∞–≥—Ä–∞–º–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {diagram_path}")
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ —Å –ø–æ–ª–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            detailed_diagram_path = os.path.join(OUTPUT_DIR, "detailed_flow.mmd")
            export_detailed_flow_diagram(
                intents=intents,
                output_path=detailed_diagram_path,
                show_slot_conditions=True,
                show_buttons=True,
                show_regex=True,
            )
            print(f"üñåÔ∏è  –î–µ—Ç–∞–ª—å–Ω–∞—è Mermaid –¥–∏–∞–≥—Ä–∞–º–º–∞: {detailed_diagram_path}")
            print(f"üëÅÔ∏è  –ü—Ä–æ—Å–º–æ—Ç—Ä Mermaid: https://mermaid.live/")
        else:
            print(f"\n‚ö†Ô∏è  Mermaid –ø—Ä–æ–ø—É—â–µ–Ω ({len(intents)} –∏–Ω—Ç–µ–Ω—Ç–æ–≤ > 1000 –ª–∏–º–∏—Ç)")
        
        # –ú—É–ª—å—Ç–∏-—Ñ–æ—Ä–º–∞—Ç–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç (–¥–ª—è –±–æ–ª—å—à–∏—Ö –≥—Ä–∞—Ñ–æ–≤)
        if MULTI_FORMAT_EXPORT_AVAILABLE:
            print()
            export_all_formats(
                intents=intents,
                transitions=transitions_full,
                output_dir=OUTPUT_DIR,
                base_name="dialog_flow",
                render_images=True,  # –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è —Å–æ–∑–¥–∞—Ç—å SVG/PNG –µ—Å–ª–∏ Graphviz —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            )
        else:
            print("\n‚ö†Ô∏è  –ú—É–ª—å—Ç–∏-—Ñ–æ—Ä–º–∞—Ç–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    # 7. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print()
    print("=" * 80)
    print("üìä –≠–¢–ê–ü 7: –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    print("=" * 80)
    print(f"üì¶ –í—Å–µ–≥–æ –∏–Ω—Ç–µ–Ω—Ç–æ–≤: {len(intents)}")
    print(f"üîó –ü–µ—Ä–µ—Ö–æ–¥–æ–≤: {len(transitions_full)}")
    
    # –ü–æ–¥—Å—á—ë—Ç –ø–æ —Ç–∏–ø–∞–º
    type_counts = {}
    for intent_id, classification in all_data.get('classifications', {}).items():
        intent_type = classification.intent_type
        type_counts[intent_type] = type_counts.get(intent_type, 0) + 1
    
    if type_counts:
        print()
        print("üìã –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º:")
        for intent_type, count in sorted(type_counts.items(), key=lambda x: -x[1]):
            percentage = (count / len(intents)) * 100
            print(f"   {intent_type}: {count} ({percentage:.1f}%)")
    
    # –ü–æ–¥—Å—á—ë—Ç –ø–æ –ø–æ–¥—Ç–∏–ø–∞–º
    if CLASSIFY_SUBTYPES:
        subtype_counts = {}
        for intent_id, classification in all_data.get('classifications', {}).items():
            if classification.subtype:
                subtype_counts[classification.subtype] = subtype_counts.get(classification.subtype, 0) + 1
        
        if subtype_counts:
            print()
            print("üè∑Ô∏è  –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø–æ–¥—Ç–∏–ø–∞–º:")
            for subtype, count in sorted(subtype_counts.items(), key=lambda x: -x[1])[:10]:
                percentage = (count / len(intents)) * 100
                print(f"   {subtype}: {count} ({percentage:.1f}%)")
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    print()
    print("=" * 80)
    print("‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–Å–ù –£–°–ü–ï–®–ù–û!")
    print("=" * 80)
    print()
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUTPUT_DIR}/")
    if ENABLE_VALIDATION:
        print(f"üìÑ –û—Ç—á—ë—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {OUTPUT_DIR}/validation_report.json")
    if RISK_ANALYSIS_AVAILABLE:
        print(f"üìÑ –û—Ç—á—ë—Ç —Ä–∏—Å–∫–æ–≤: {OUTPUT_DIR}/risk_analysis.json")
    if EXPORT_DIAGRAMS:
        print(f"üñåÔ∏è  –î–∏–∞–≥—Ä–∞–º–º—ã:")
        if len(intents) <= 1000:
            print(f"   ‚Ä¢ Mermaid: {OUTPUT_DIR}/graph.mmd, detailed_flow.mmd")
        print(f"   ‚Ä¢ Graphviz: {OUTPUT_DIR}/dialog_flow.dot (.svg)")
        print(f"   ‚Ä¢ GraphML (yEd): {OUTPUT_DIR}/dialog_flow.graphml")
        print(f"   ‚Ä¢ GEXF (Gephi): {OUTPUT_DIR}/dialog_flow.gexf")
        print(f"   ‚Ä¢ JSON (web): {OUTPUT_DIR}/dialog_flow_*.json")
    print()
    print("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ—Å–º–æ—Ç—Ä—É –±–æ–ª—å—à–∏—Ö –¥–∏–∞–≥—Ä–∞–º–º:")
    print("   ‚Ä¢ Gephi (https://gephi.org/) - –ª—É—á—à–∏–π –¥–ª—è 1000+ —É–∑–ª–æ–≤")
    print("   ‚Ä¢ yEd (https://www.yworks.com/yed) - —Ö–æ—Ä–æ—à –¥–ª—è GraphML")
    print("   ‚Ä¢ Cytoscape (https://cytoscape.org/) - –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
    print()
    
    return 0

if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print()
        print()
        print("‚ö†Ô∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print()
        print()
        print("=" * 80)
        print("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê")
        print("=" * 80)
        print(f"–¢–∏–ø: {type(e).__name__}")
        print(f"–°–æ–æ–±—â–µ–Ω–∏–µ: {e}")
        print()
        import traceback
        traceback.print_exc()
        print()
        sys.exit(1)
